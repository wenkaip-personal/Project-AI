{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Set Flow Matching Posterior Estimation\n",
    "\n",
    "This notebook demonstrates how to perform deep set flow matching posterior estimation (DeepSet FMPE) with LAMPE, focusing on handling sets of parameters and observations in a permutation-invariant manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import zuko\n",
    "\n",
    "from itertools import islice\n",
    "from lampe.data import JointLoader\n",
    "from inference_multisets import DeepSetFMPE, DeepSetFMPELoss\n",
    "from lampe.plots import corner, mark_point, nice_rc\n",
    "from lampe.utils import GDStep\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator\n",
    "\n",
    "We define a simple simulator function that generates observations based on sets of parameters. This function is designed to be permutation-invariant with respect to the input sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7399,  0.9650, -0.2231])\n",
      "tensor([-0.9622, -0.9301])\n"
     ]
    }
   ],
   "source": [
    "LABELS = [r'$\\theta_1$', r'$\\theta_2$', r'$\\theta_3$']\n",
    "LOWER = -torch.ones(3)\n",
    "UPPER = torch.ones(3)\n",
    "\n",
    "prior = zuko.distributions.BoxUniform(LOWER, UPPER)\n",
    "\n",
    "def simulator(theta: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.stack([\n",
    "        theta[..., 0] + theta[..., 1] * theta[..., 2],\n",
    "        theta[..., 0] * theta[..., 1] + theta[..., 2],\n",
    "    ], dim=-1)\n",
    "\n",
    "    return x + 0.05 * torch.randn_like(x)\n",
    "\n",
    "theta = prior.sample()\n",
    "x = simulator(theta)\n",
    "\n",
    "print(theta, x, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train our DeepSetFMPE model using a standard neural network training routine. The goal is to learn a regression network that approximates a vector field for permutation-invariant sets of parameters and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSetFMPE(\n",
      "  (net): Sequential()\n",
      "  (activation): ELU(alpha=1.0)\n",
      "  (phi_theta): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "  )\n",
      "  (phi_x): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "  )\n",
      "  (time_embedding): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (rho): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [01:33<00:00,  1.36epoch/s, loss=0.351]\n"
     ]
    }
   ],
   "source": [
    "loader = JointLoader(prior, simulator, batch_size=256, vectorized=True)\n",
    "\n",
    "estimator = DeepSetFMPE(3, 2, hidden_features=64, num_hidden_layers=5, activation=nn.ELU)\n",
    "print(estimator)\n",
    "\n",
    "loss = DeepSetFMPELoss(estimator)\n",
    "optimizer = optim.Adam(estimator.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 128)\n",
    "step = GDStep(optimizer, clip=1.0)  # gradient descent step with gradient clipping\n",
    "\n",
    "estimator.train()\n",
    "\n",
    "for epoch in (bar := trange(128, unit='epoch')):\n",
    "    losses = []\n",
    "\n",
    "    for theta, x in islice(loader, 256):  # 256 batches per epoch\n",
    "        losses.append(step(loss(theta, x)))\n",
    "\n",
    "    bar.set_postfix(loss=torch.stack(losses).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that we have an estimator of the vector field, we can sample from the normalizing flow it induces for sets of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "got 2 tensors and 1 gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m estimator\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     log_p \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_star\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_star\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     samples \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mflow(x_star)\u001b[38;5;241m.\u001b[39msample((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m14\u001b[39m,))\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/distributions.py:110\u001b[0m, in \u001b[0;36mNormalizingFlow.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 110\u001b[0m     z, ladj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_and_ladj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     ladj \u001b[38;5;241m=\u001b[39m _sum_rightmost(ladj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mlog_prob(z) \u001b[38;5;241m+\u001b[39m ladj\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/transforms.py:1009\u001b[0m, in \u001b[0;36mFreeFormJacobianTransform.call_and_ladj\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dx, trace \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_scale\n\u001b[1;32m   1008\u001b[0m ladj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1009\u001b[0m y, ladj \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mladj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, ladj \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_scale)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/utils.py:319\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(f, x, t0, t1, phi, atol, rtol)\u001b[0m\n\u001b[1;32m    315\u001b[0m t1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(t1, dtype\u001b[38;5;241m=\u001b[39mx0\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t0\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t1\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be scalars\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[43mAdaptiveCheckpointAdjoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(x):\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x1\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/utils.py:422\u001b[0m, in \u001b[0;36mAdaptiveCheckpointAdjoint.forward\u001b[0;34m(ctx, settings, f, x, t0, t1, *phi)\u001b[0m\n\u001b[1;32m    419\u001b[0m dt \u001b[38;5;241m=\u001b[39m sign \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;28mabs\u001b[39m(dt), \u001b[38;5;28mabs\u001b[39m(t1 \u001b[38;5;241m-\u001b[39m t))\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     y, error \u001b[38;5;241m=\u001b[39m \u001b[43mdopri45\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m     tolerance \u001b[38;5;241m=\u001b[39m atol \u001b[38;5;241m+\u001b[39m rtol \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mabs\u001b[39m(x), \u001b[38;5;28mabs\u001b[39m(y))\n\u001b[1;32m    424\u001b[0m     error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(error \u001b[38;5;241m/\u001b[39m tolerance)\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/utils.py:340\u001b[0m, in \u001b[0;36mdopri45\u001b[0;34m(f, x, t, dt, error)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdopri45\u001b[39m(\n\u001b[1;32m    328\u001b[0m     f: Callable[[Tensor, Tensor], Tensor],\n\u001b[1;32m    329\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m     error: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Tuple[Tensor, Tensor]]:\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies one step of the Dormand-Prince method.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    Wikipedia:\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m        https://wikipedia.org/wiki/Dormand-Prince_method\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     k1 \u001b[38;5;241m=\u001b[39m dt \u001b[38;5;241m*\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     k2 \u001b[38;5;241m=\u001b[39m dt \u001b[38;5;241m*\u001b[39m f(t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m dt, x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m k1)\n\u001b[1;32m    342\u001b[0m     k3 \u001b[38;5;241m=\u001b[39m dt \u001b[38;5;241m*\u001b[39m f(t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m dt, x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;241m*\u001b[39m k2)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/utils.py:312\u001b[0m, in \u001b[0;36modeint.<locals>.<lambda>\u001b[0;34m(t, x)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([y\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m    311\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m pack(x)\n\u001b[0;32m--> 312\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t, x: pack(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    314\u001b[0m t0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(t0, dtype\u001b[38;5;241m=\u001b[39mx0\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    315\u001b[0m t1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(t1, dtype\u001b[38;5;241m=\u001b[39mx0\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx0\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/zuko/transforms.py:1000\u001b[0m, in \u001b[0;36mFreeFormJacobianTransform.call_and_ladj.<locals>.f_aug\u001b[0;34m(t, x, ladj)\u001b[0m\n\u001b[1;32m    997\u001b[0m     dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(t, x)\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact:\n\u001b[0;32m-> 1000\u001b[0m     jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m     trace \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi...i\u001b[39m\u001b[38;5;124m'\u001b[39m, jacobian)\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/autograd/__init__.py:407\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    398\u001b[0m             t_outputs,\n\u001b[1;32m    399\u001b[0m             gO,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m             accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    405\u001b[0m         )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap_internals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_none_pass_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     result \u001b[38;5;241m=\u001b[39m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    412\u001b[0m         t_outputs,\n\u001b[1;32m    413\u001b[0m         grad_outputs_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    419\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/_vmap_internals.py:223\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    221\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    225\u001b[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/autograd/__init__.py:397\u001b[0m, in \u001b[0;36mgrad.<locals>.vjp\u001b[0;34m(gO)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: got 2 tensors and 1 gradients"
     ]
    }
   ],
   "source": [
    "theta_star = prior.sample()\n",
    "x_star = simulator(theta_star)\n",
    "\n",
    "estimator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_p = estimator.flow(x_star).log_prob(theta_star)\n",
    "    samples = estimator.flow(x_star).sample((2**14,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the posterior samples to assess the quality of our DeepSetFMPE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(nice_rc(latex=True))  # nicer plot settings\n",
    "\n",
    "fig = corner(\n",
    "    samples,\n",
    "    smooth=2,\n",
    "    domain=(LOWER, UPPER),\n",
    "    labels=LABELS,\n",
    "    legend=r'$p_\\phi(\\theta | x^*)$',\n",
    "    figsize=(4.8, 4.8),\n",
    ")\n",
    "\n",
    "mark_point(fig, theta_star)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
