{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting Single Sample Analysis\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this AI research project, we're developing a general posterior estimator for complex scientific models. Our approach involves building the entire pipeline from the simplest possible case - overfitting on a single sample - and gradually increasing complexity. This methodical process allows us to ensure each component works as expected before adding more layers.\n",
    "\n",
    "The main goals of this notebook are:\n",
    "\n",
    "1. To demonstrate the process of overfitting on a single sample using two approaches:\n",
    "   a) Flow Matching (from flow_matching_single_example.py)\n",
    "   b) Conditional Case (from pipeline_small.py)\n",
    "\n",
    "2. To verify that the implemented general posterior estimator in the conditional case functions properly.\n",
    "\n",
    "3. To prepare for future comparisons with \"ground truth\" models using MCMC or nested sampling for performance evaluation.\n",
    "\n",
    "By starting with overfitting on a single sample, we can debug and test each level of the pipeline, ensuring a solid foundation for more complex implementations.\n",
    "\n",
    "## 2. Flow Matching Single Example\n",
    "\n",
    "### 2.1 Code Explanation\n",
    "\n",
    "The flow_matching_single_example.py script implements a flow matching algorithm to overfit on a single data point from the \"moons\" dataset. Let's break down the main components:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "from zuko.utils import odeint\n",
    "\n",
    "# ... (rest of the imports)\n",
    "\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, hidden_features=[64, 64]):\n",
    "        layers = []\n",
    "        for a, b in zip((in_features, *hidden_features), (*hidden_features, out_features)):\n",
    "            layers.extend([nn.Linear(a, b), nn.ELU()])\n",
    "        super().__init__(*layers[:-1])\n",
    "\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, features, freqs=3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.net = MLP(2 * freqs + features, features, **kwargs)\n",
    "        self.register_buffer('freqs', torch.arange(1, freqs + 1) * torch.pi)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        t = self.freqs * t[..., None]\n",
    "        t = torch.cat((t.cos(), t.sin()), dim=-1)\n",
    "        t = t.expand(*x.shape[:-1], -1)\n",
    "        return self.net(torch.cat((t, x), dim=-1))\n",
    "\n",
    "    # ... (rest of the CNF class methods)\n",
    "\n",
    "class FlowMatchingLoss(nn.Module):\n",
    "    def __init__(self, v):\n",
    "        super().__init__()\n",
    "        self.v = v\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.rand_like(x[..., 0, None])\n",
    "        z = torch.randn_like(x)\n",
    "        y = (1 - t) * x + (1e-4 + (1 - 1e-4) * t) * z\n",
    "        u = (1 - 1e-4) * z - x\n",
    "        return (self.v(t.squeeze(-1), y) - u).square().mean()\n",
    "\n",
    "# Main training loop\n",
    "if __name__ == '__main__':\n",
    "    flow = CNF(2, hidden_features=[64] * 3)\n",
    "    loss = FlowMatchingLoss(flow)\n",
    "    optimizer = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "\n",
    "    noise_levels = [0.01, 0.05, 0.1]\n",
    "    for i, noise in enumerate(noise_levels):\n",
    "        data, _ = make_moons(1, noise=noise)\n",
    "        data = torch.from_numpy(data).float()\n",
    "        \n",
    "        batch_size = 32\n",
    "        data_batch = data.expand(batch_size, -1, -1)\n",
    "\n",
    "        for epoch in tqdm(range(10000), ncols=88, desc=f'Training with noise={noise}'):\n",
    "            x = data_batch\n",
    "            loss(x).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Sampling and visualization\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(10, 2)\n",
    "            x = flow.decode(z)\n",
    "\n",
    "        plt.figure(figsize=(4.8, 4.8), dpi=150)\n",
    "        plt.scatter(*data.T, color='red', label='Ground Truth')\n",
    "        plt.scatter(*x.T, color='blue', alpha=0.5, label='Samples')\n",
    "        plt.legend()\n",
    "        plt.xlim(-1.5, 2.5)\n",
    "        plt.ylim(-1, 1.5)\n",
    "        plt.title(f'Overfitting on Single Example with Noise={noise}')\n",
    "        plt.savefig(f'experiments/plots_fm/moons_fm_single_example_noise_{noise}.pdf')\n",
    "\n",
    "        # Log-likelihood calculation\n",
    "        with torch.no_grad():\n",
    "            log_p = flow.log_prob(data)\n",
    "        print(f'Log probability for noise {noise}: {log_p.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the MLP, CNF, and FlowMatchingLoss classes, then implements the training loop, sampling, and visualization.\n",
    "### 2.2 Results Analysis\n",
    "Let's look at the running outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with noise=0.05: 100%|█████████████████| 10000/10000 [00:04<00:00, 2187.57it/s]\n",
    "Log probability for noise 0.05: 7.608288764953613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outputs show that:\n",
    "\n",
    "1. The training completed 10,000 epochs for the noise level of 0.05.\n",
    "2. The log probability for this noise level is 7.608288764953613.\n",
    "\n",
    "This high log probability indicates that the model has successfully overfit to the single example, as it assigns a very high likelihood to the training point.\n",
    "### 2.3 Plot Interpretation\n",
    "Let's analyze the plot \"Overfitting on Single Example with Noise=0.05\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Overfitting on Single Example with Noise=0.05](plots_fm/moons_fm_single_example_noise_0.05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot:\n",
    "\n",
    "1. The red dot represents the ground truth (original data point).\n",
    "2. The blue dots represent samples generated by the model.\n",
    "3. We can observe that the generated samples cluster tightly around the ground truth, indicating successful overfitting.\n",
    "\n",
    "The tight clustering shows that the model has learned to generate points very close to the single training example, rather than capturing the general shape of the \"moons\" distribution. This is exactly what we want in this overfitting exercise.\n",
    "\n",
    "## 3. Conditional Case\n",
    "\n",
    "### 3.1 Code Explanation\n",
    "The pipeline_small.py script implements a conditional flow matching approach. Let's break down the key components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import corner\n",
    "import numpy as np\n",
    "from lampe.plots import nice_rc\n",
    "from lampe.utils import GDStep\n",
    "from lampe.inference import FMPE, FMPELoss\n",
    "from generate_burst_data import simulate_burst\n",
    "\n",
    "# ... (rest of the imports and utility functions)\n",
    "\n",
    "# Initialize models, loss functions, and optimizers\n",
    "estimator = FMPE(theta_dim=1 * max_ncomp, x_dim=1000, freqs=5).to(device)\n",
    "loss = FMPELoss(estimator)\n",
    "optimizer = optim.Adam(estimator.parameters(), lr=1e-4)\n",
    "step = GDStep(optimizer, clip=1.0)\n",
    "\n",
    "# Generate a single sample for overfitting\n",
    "ncomp = 2\n",
    "burstparams = generate_burst_params(ncomp)\n",
    "ymodel, ycounts = simulate_burst(time, ncomp, burstparams, ybkg*10, return_model=True, noise_type='gaussian')\n",
    "fixed_t0 = torch.from_numpy(burstparams[:ncomp]).float().to(device)\n",
    "fixed_x = torch.from_numpy(ycounts).float().to(device)\n",
    "\n",
    "# Transform the time series data using Fourier Transform\n",
    "import torch.fft as fft\n",
    "fixed_x_fft = torch.abs(fft.fft(fixed_x))\n",
    "\n",
    "# Training loop for overfitting\n",
    "estimator.train()\n",
    "\n",
    "batch_size = 1024\n",
    "fixed_t0_batch = fixed_t0.repeat(batch_size, 1)\n",
    "fixed_x_fft_batch = fixed_x_fft.repeat(batch_size, 1)\n",
    "\n",
    "num_epochs = 50000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = loss(fixed_t0_batch, fixed_x_fft_batch)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Overfitting Epoch {epoch+1}, Loss: {loss_value.item()}\")\n",
    "\n",
    "# Evaluation after overfitting\n",
    "estimator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_samples = 1000\n",
    "    samples_t0_given_x_N = estimator.flow(fixed_x_fft_batch).sample((num_samples,))\n",
    "    \n",
    "    mean_sample = samples_t0_given_x_N.mean(dim=0)\n",
    "    std_sample = samples_t0_given_x_N.std(dim=0)\n",
    "    \n",
    "    batch_mean_sample = samples_t0_given_x_N.mean(dim=[0, 1])\n",
    "    batch_std_sample = samples_t0_given_x_N.std(dim=[0, 1])\n",
    "    \n",
    "    print(\"Fixed Input t0:\", fixed_t0_batch[0])\n",
    "    print(\"Mean of sampled t0s from posterior after overfitting:\", mean_sample)\n",
    "    print(\"Standard deviation of sampled t0s from posterior after overfitting:\", std_sample)\n",
    "    print(\"Batch mean of sampled t0s from posterior after overfitting:\", batch_mean_sample)\n",
    "    print(\"Batch standard deviation of sampled t0s from posterior after overfitting:\", batch_std_sample)\n",
    "\n",
    "    # Plotting the samples using corner\n",
    "    figure = corner.corner(samples_t0_given_x_N.view(-1, samples_t0_given_x_N.size(-1)).cpu().numpy(), \n",
    "                           labels=[f\"t0_{i}\" for i in range(samples_t0_given_x_N.size(-1))],\n",
    "                           truths=fixed_t0_batch[0].cpu().numpy(), title=\"Posterior Samples vs Fixed Input t0\")\n",
    "    figure.savefig('plots_small/posterior_samples_fixed_t0_corner_fft_overfit.png')\n",
    "    plt.close(figure)\n",
    "\n",
    "    # Plotting the overfitting loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(overfit_loss_values, label='Overfitting Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Overfitting Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots_small/loss_curve_fft_overfit.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plotting the conditioning data (time series)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, ycounts, label='Conditioning Time Series Data')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Conditioning Time Series Data')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots_small/conditioning_data_time_series.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up the FMPE model, generates a single burst data sample, transforms it using FFT, and then trains the model to overfit on this single sample. It then evaluates the model and generates various plots for analysis.\n",
    "\n",
    "### 3.2 Results Analysis\n",
    "Let's examine the running outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using device: cuda\n",
    "\n",
    "Overfitting Epoch 1, Loss: 15840.744140625\n",
    "\n",
    "Overfitting Epoch 1001, Loss: 0.9846465587615967\n",
    "\n",
    "Overfitting Epoch 2001, Loss: 1.0064780712127686\n",
    "\n",
    "...\n",
    "\n",
    "Overfitting Epoch 48001, Loss: 0.2650120258331299\n",
    "\n",
    "Overfitting Epoch 49001, Loss: 0.2915732264518738\n",
    "\n",
    "Training completed in 95.00 seconds.\n",
    "\n",
    "Fixed Input t0: tensor([0.5347, 0.3227], device='cuda:0')\n",
    "\n",
    "Mean of sampled t0s from posterior after overfitting: tensor([[0.5523, 0.3232],\n",
    "        [0.5653, 0.3281],\n",
    "        [0.5659, 0.3096],\n",
    "        ...,\n",
    "        [0.5642, 0.3147],\n",
    "        [0.5519, 0.3207],\n",
    "        [0.5561, 0.3293]], device='cuda:0')\n",
    "\n",
    "Standard deviation of sampled t0s from posterior after overfitting: tensor([[0.2339, 0.2265],\n",
    "        [0.2301, 0.2214],\n",
    "        [0.2270, 0.2220],\n",
    "        ...,\n",
    "        [0.2352, 0.2259],\n",
    "        [0.2213, 0.2184],\n",
    "        [0.2293, 0.2329]], device='cuda:0')\n",
    "\n",
    "Batch mean of sampled t0s from posterior after overfitting: tensor([0.5579, 0.3230], device='cuda:0')\n",
    "\n",
    "Batch standard deviation of sampled t0s from posterior after overfitting: tensor([0.2292, 0.2272], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show:\n",
    "\n",
    "1. The training completed 50,000 epochs.\n",
    "2. The loss decreased from 15840.744140625 to around 0.2915732264518738.\n",
    "3. The fixed input t0 values are [0.5347, 0.3227].\n",
    "4. The batch mean of sampled t0s [0.5579, 0.3230] is close to the input.\n",
    "5. The batch standard deviation [0.2292, 0.2272] is relatively high.\n",
    "\n",
    "These results suggest that while the model has learned to generate samples close to the input on average, there's still significant variability in the outputs.\n",
    "\n",
    "### 3.3 Plot Interpretation\n",
    "Let's analyze the generated plots:\n",
    "\n",
    "1. Posterior Samples vs Fixed Input t0 (Corner Plot):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Posterior Samples vs Fixed Input t0 (Corner Plot)](plots_small/posterior_samples_fixed_t0_corner_fft_overfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corner plot shows:\n",
    "\n",
    "* The distribution of sampled t0 values in 2D.\n",
    "* Blue lines represent the true input t0 values.\n",
    "* Contours represent the 68%, 95%, and 99.7% credible regions.\n",
    "* Histograms on the diagonal show the marginal distributions for each t0.\n",
    "\n",
    "2. Overfitting Loss Curve:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Overfitting Loss Curve](plots_small/loss_curve_fft_overfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows how the loss decreased over the training epochs.\n",
    "\n",
    "3. Conditioning Time Series Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Conditioning Time Series Data](plots_small/conditioning_data_time_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the single burst data sample used for conditioning.\n",
    "### 3.4 Issues with Overfitting Procedure\n",
    "Despite the long training process, there are still some issues with the overfitting:\n",
    "\n",
    "1. Spread of Samples: The posterior samples are more spread out than expected for perfect overfitting. Ideally, they should cluster very tightly around the true values.\n",
    "2. Credible Regions: Many samples lie outside the innermost credible region. For true overfitting, we'd expect almost all samples to be within the 68% credible region.\n",
    "3. Uncertainty: The relatively high standard deviations (0.2292 and 0.2272) indicate that the model is still uncertain about the true values, despite seeing only one example repeatedly.\n",
    "\n",
    "These issues suggest that the model might be struggling to fully capture the complexity of the single example, or that the training process could be further optimized.\n",
    "## 4. Conclusion\n",
    "This notebook has demonstrated the process of overfitting on a single sample using both flow matching and conditional approaches. These experiments represent the initial steps in building a robust pipeline for a general posterior estimator in complex scientific models. The results show that the model is still uncertain about the true values, despite seeing only one example repeatedly. This suggests that further optimization of the training process is necessary to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key takeaways:\n",
    "\n",
    "1. The flow matching example showed tighter clustering around the true value, indicating successful overfitting on a single sample.\n",
    "\n",
    "2. The conditional case, while showing promising results, still exhibits some spread in its posterior samples. This suggests that further refinement may be needed in the model architecture, loss function, or training process.\n",
    "\n",
    "3. The issues identified in the conditional case provide valuable insights for improving the general posterior estimator. Addressing these challenges will be crucial as we move towards more complex implementations.\n",
    "\n",
    "Next steps in the research project:\n",
    "\n",
    "1. Refine the conditional case to achieve tighter overfitting on a single sample. This may involve:\n",
    "   - Experimenting with different model architectures\n",
    "   - Adjusting the learning rate or using learning rate schedules\n",
    "   - Increasing the number of training epochs\n",
    "   - Exploring alternative loss functions\n",
    "\n",
    "2. Gradually increase the complexity of the input data and model:\n",
    "   - Test with multiple burst components\n",
    "   - Introduce variability in the background noise\n",
    "   - Experiment with different noise types (e.g., Poisson noise)\n",
    "\n",
    "3. Implement and compare with \"ground truth\" models using MCMC or nested sampling:\n",
    "   - Develop MCMC and nested sampling implementations for the burst model\n",
    "   - Compare the posterior distributions obtained from these methods with our flow-based approach\n",
    "\n",
    "4. Evaluate the performance of the general posterior estimator against these benchmark methods:\n",
    "   - Compare computational efficiency\n",
    "   - Assess accuracy of posterior estimates\n",
    "   - Analyze the ability to capture multi-modal or complex posterior distributions\n",
    "\n",
    "By continuing this methodical approach of testing and refinement at each level of complexity, we can build a robust and accurate general posterior estimator for complex scientific models.\n",
    "\n",
    "## 5. Further Investigations\n",
    "\n",
    "Based on our current results, here are some specific areas we could investigate to improve the conditional case:\n",
    "\n",
    "1. Model Capacity: The current FMPE model might not have sufficient capacity to capture the complexity of the burst data. We could experiment with:\n",
    "   - Increasing the number of layers or neurons in the neural network\n",
    "   - Using more sophisticated architectures like residual networks or attention mechanisms\n",
    "\n",
    "2. Training Dynamics: The training process might be sub-optimal. We could try:\n",
    "   - Implementing gradient clipping to handle potential exploding gradients\n",
    "   - Using a learning rate scheduler to adjust the learning rate during training\n",
    "   - Exploring different optimizers (e.g., AdamW, RMSprop)\n",
    "\n",
    "3. Data Representation: The current FFT representation might not be optimal. We could investigate:\n",
    "   - Different data transformations (e.g., wavelet transforms)\n",
    "   - Normalizing or scaling the input data differently\n",
    "\n",
    "4. Loss Function: The current loss function might not be ideal for this specific problem. We could:\n",
    "   - Experiment with different variants of the flow matching loss\n",
    "   - Incorporate additional regularization terms\n",
    "\n",
    "5. Sampling Process: The current sampling process might not be capturing the true posterior effectively. We could:\n",
    "   - Increase the number of samples drawn\n",
    "   - Implement Markov Chain Monte Carlo (MCMC) sampling on top of the flow-based model\n",
    "\n",
    "6. Numerical Stability: There might be numerical stability issues affecting the results. We should:\n",
    "   - Check for any NaN or infinity values during training\n",
    "   - Implement robust handling of edge cases in the model\n",
    "\n",
    "By systematically addressing these areas, we can work towards improving the overfitting performance on a single sample in the conditional case, setting a strong foundation for more complex scenarios in our general posterior estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
